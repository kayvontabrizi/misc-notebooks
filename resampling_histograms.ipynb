{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a2c9978",
   "metadata": {},
   "source": [
    "# Investigations into resampling PMFs\n",
    "\n",
    "<br>\n",
    "\n",
    "### Uncertainty in a Sampling Distribution\n",
    "Given IID samples drawn from a PMF, what is the uncertainty in the sampled PMF? Below we use the bootstrap to compute the uncertainty across a PMFs in a discrete 1-D state space. By examining the behavior, we guess at an analytic description of this uncertainty. Finally, we examine the propagation of that uncertainty into the space of log probabilities (with free energy calculations in mind).\n",
    "\n",
    "<br>\n",
    "\n",
    "### Blocked Bootstrapping\n",
    "The blocked boostrap extends the idea of bootstrapping to a set of correlated samples (a data \"signal\"). In the case of free energy calculations, samples are often correlated (if e.g. generated by MC or MD). We apply a blocked-bootstrap to compute PMF uncertainties for these correlated samples.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Resources\n",
    "- [The Uncertainty in the Estimate of the Discrete Probability](https://apt.cs.manchester.ac.uk/ftp/pub/ai/jls/CS2411/prob97/node11.html)\n",
    "\n",
    "\n",
    "###### Note to self: Some of the below fails stochastically. Just rerun the failed bit!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b4ce38",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaefb9c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## imports & configuration\n",
    "\n",
    "# standard imports\n",
    "\n",
    "# custom imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from scipy.spatial.transform import Rotation\n",
    "import tqdm.notebook\n",
    "\n",
    "# inline plots\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "# %matplotlib notebook\n",
    "\n",
    "# jupyter theme\n",
    "try:\n",
    "    import jupyterthemes as jt\n",
    "    jt.jtplot.style()\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c0492e",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae3ae0b",
   "metadata": {},
   "source": [
    "## Uncertainty in a Sampling Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54304c60",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# prep and seed random number generator\n",
    "rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38729163",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## sample from a n-simplex\n",
    "\n",
    "# sample n points from a d-dimensional simplex\n",
    "def simplex(n, d):\n",
    "    exps = -np.log(rng.random((n, d))) # draw from exp(-x)\n",
    "    return exps/exps.sum(axis=-1)[:, None] # normalize sum to 1\n",
    "\n",
    "# rotate simplex points and plot (should be a uniform distribution)\n",
    "plt.figure(figsize=(6, 3))\n",
    "r_mat = Rotation.from_euler('z', np.pi/4).as_matrix()[:2, :2]\n",
    "plt.hist(r_mat.dot(simplex(100000, 2).T)[0]*np.sqrt(0.5)+0.5, bins=100, density=True)\n",
    "plt.xlabel(\"parameter along $(x + y = 1)$\")\n",
    "plt.ylabel(\"density\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca6ffda",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## compute boostrap-based uncertainty in discrete distribution\n",
    "\n",
    "# configuration\n",
    "n_samples = 1000 # number of samples\n",
    "d = 5 # number of states\n",
    "\n",
    "# generate samples from random distribution with d states\n",
    "states = np.arange(d)\n",
    "ps = simplex(1, d)[0]\n",
    "samples = rng.choice(states, n_samples, p=ps)\n",
    "_, counts = np.unique(samples, return_counts=True)\n",
    "ps = counts/n_samples\n",
    "\n",
    "# bootstrap to compute probability uncertainties\n",
    "errors = np.array([\n",
    "    scipy.stats.bootstrap(samples[None, :], lambda _samples: (_samples == state).sum()/len(_samples), vectorized=False).standard_error\n",
    "    for state in states\n",
    "])\n",
    "\n",
    "# create histogram\n",
    "plt.bar(states, ps, yerr=errors)\n",
    "plt.xlabel(\"state\")\n",
    "plt.ylabel(\"density\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c18aba",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## method to compute bootstrap-based uncertainty estimates\n",
    "def compute_uncertainty_in_distribution(n_samples, d):\n",
    "    # generate samples from random distribution with d states\n",
    "    states = np.arange(d)\n",
    "    ps = simplex(1, d)[0]\n",
    "    samples = rng.choice(states, n_samples, p=ps)\n",
    "    _, counts = np.unique(samples, return_counts=True)\n",
    "    ps = counts/n_samples\n",
    "\n",
    "    # bootstrap to compute probability uncertainties\n",
    "    errors = np.array([\n",
    "        scipy.stats.bootstrap(samples[None, :], lambda _samples: (_samples == state).sum()/len(_samples), vectorized=False).standard_error\n",
    "        for state in states\n",
    "    ])\n",
    "\n",
    "    # return sampling distribution and associated uncertainties\n",
    "    return ps, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ce399",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# compute errors for a variety of sample sizes\n",
    "ns = np.arange(200, 1100, 100)\n",
    "pses, errorses = zip(*[compute_uncertainty_in_distribution(n, 6) for n in ns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d84d68",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## examine relationship between errors and sample size\n",
    "\n",
    "# initialize figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "# plot the constant (== error**2 / probability) averaged across states\n",
    "scalings = (np.stack(errorses)**2/np.stack(pses))\n",
    "axes[0].errorbar(ns, scalings.mean(axis=-1), yerr=scalings.std(axis=-1)/np.sqrt(scalings.shape[1]))\n",
    "axes[0].scatter(ns, scalings.mean(axis=-1))\n",
    "axes[0].set_xlabel(\"# of samples\")\n",
    "axes[0].set_ylabel(\"$\\sigma(p_i)^2$ / $p_i$\")\n",
    "\n",
    "# recognize 1/n shape, plot full error relationship\n",
    "axes[1].axhline(1-(1/scalings.shape[1]), color='k', ls='--', lw=0.5)\n",
    "axes[1].plot(ns, scalings.mean(axis=-1)*ns, \"o-\")\n",
    "axes[1].set_xlabel(\"# of samples\")\n",
    "axes[1].set_ylabel(\"$n * \\sigma(p_i)^2$ / $p_i$\")\n",
    "\n",
    "# display figure\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f77721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduce relationship between probability, # of states, and # of samples\n",
    "std_err = lambda p, d, n: np.sqrt(p*(1-(1/d))/n) # !avg(ps) == 1-(1/d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b70fdc2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# compare empirical and analytic uncertainties\n",
    "plt.plot(*[(std_err(0, d, n_samples), std_err(1, d, n_samples))]*2, c='k', ls='--', lw=1)\n",
    "plt.scatter(errors, [std_err(p, d, n_samples) for p in ps])\n",
    "plt.xlabel(\"empirical uncertainty\")\n",
    "plt.ylabel(\"analytic uncertainty\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1635d007",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeef354",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## bootstrap the -ln(p) uncertainty\n",
    "\n",
    "# configuration\n",
    "n_samples = 10000 # number of samples\n",
    "d = 10 # number of states\n",
    "\n",
    "# generate samples from random distribution with d states\n",
    "states = np.arange(d)\n",
    "ps = simplex(1, d)[0]\n",
    "samples = rng.choice(states, n_samples, p=ps)\n",
    "_, counts = np.unique(samples, return_counts=True)\n",
    "ps = counts/n_samples\n",
    "\n",
    "# bootstrap to compute probability uncertainties\n",
    "boots = np.array([\n",
    "    scipy.stats.bootstrap(samples[None, :], lambda _samples: -np.log((_samples == state).sum()/len(_samples)), vectorized=False)\n",
    "    for state in states\n",
    "])\n",
    "errors = np.array([b.standard_error for b in boots])\n",
    "\n",
    "# plot -logPs with errors\n",
    "plt.bar(states, -np.log(ps), yerr=errors)\n",
    "plt.xlabel(\"state\")\n",
    "plt.ylabel(\"-logP\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b65feb",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# compare empirical and analytic -logp uncertainties\n",
    "plt.plot(*[(0, std_err(ps.min(), d, n_samples)/ps.min())]*2, c='k', ls='--', lw=1)\n",
    "plt.scatter(errors, np.array([std_err(p, d, n_samples) for p in ps])/ps)\n",
    "plt.xlabel(\"empirical uncertainty\")\n",
    "plt.ylabel(\"analytic uncertainty\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf03b39",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# slowly compute dGs from i -> i+1 (takes like 2-3 minutes)\n",
    "dG_errs = np.array([\n",
    "    scipy.stats.bootstrap(\n",
    "        samples[None, :],\n",
    "        lambda _samples: np.diff(-np.log(np.unique(_samples, return_counts=True)[1]/len(_samples)))[i],\n",
    "        vectorized=False\n",
    "    ).standard_error\n",
    "    for i in range(len(states)-1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af912a79",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## compare empirical and analytic dG errors\n",
    "\n",
    "# analytic dG error computation\n",
    "_err = (np.array([std_err(p, d, n_samples) for p in ps])/ps)\n",
    "_dG_errs = np.sqrt((_err**2)[:-1]+(_err**2)[1:])\n",
    "_dG_errs *= np.sqrt(d/(d-1)) # not sure why this second factor is necessary\n",
    "\n",
    "# compare empirical and analytic uncertainties\n",
    "plt.plot(*[(0, max(dG_errs.max(), _dG_errs.max()))]*2, c='k', ls='--', lw=1)\n",
    "plt.scatter(dG_errs, _dG_errs)\n",
    "plt.xlabel(\"empirical uncertainty\")\n",
    "plt.ylabel(\"analytic uncertainty\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e802c",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# method to compute i->i+1 dG error from sampling distribution (and # of samples)\n",
    "def dG_err(ps, n_samples):\n",
    "    d = len(ps)\n",
    "    p_err = np.array([np.sqrt(p*(1-(1/d))/n_samples) for p in ps])\n",
    "    logp_err = p_err/ps\n",
    "    correction = np.sqrt(d/(d-1))\n",
    "    dg_err = np.sqrt((logp_err**2)[:-1]+(logp_err**2)[1:])*correction\n",
    "    return p_err, dg_err # returns standard error in p and -log(p_{i+1}/p_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a8bf9f",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d8ce98",
   "metadata": {},
   "source": [
    "## Blocked Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518e352d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# generate random samples from random distribution\n",
    "n_samples, ps = 10000, simplex(1, 20)[0]\n",
    "samples = rng.choice(np.arange(len(ps)), size=n_samples, p=ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034c2e2b",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# blocked bootstrap settings\n",
    "n_blocks_per_signal = 1000 # THIS IS BLOCK SIZE\n",
    "n_resamples = 9999 # number of resampled signals to construct\n",
    "n_blocks = 1000 # should be a multiple of n_blocks_per_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b596fa",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# generate blocks from signal\n",
    "block_len = len(samples)//n_blocks_per_signal\n",
    "padded_samples = np.concatenate([\n",
    "    samples, # support wrap around by adding an extra block length:\n",
    "    samples[:block_len+(-len(samples)%block_len)]\n",
    "])\n",
    "blocks = np.concatenate([\n",
    "    np.split(padded_samples[:-block_len], (len(padded_samples)-1)//block_len)[:-1]\n",
    "    for i in range(n_blocks//n_blocks_per_signal) for j in rng.integers(block_len, size=1)\n",
    "])\n",
    "signals = np.concatenate(\n",
    "    blocks[\n",
    "        rng.integers(len(blocks), size=n_blocks_per_signal*n_resamples)\n",
    "    ].reshape(n_blocks_per_signal, n_resamples, -1) # indexing blocks is the slow step of this cell\n",
    ", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547411e9",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# compile counts for each signal\n",
    "indices, truth = np.unique(samples, return_counts=True)\n",
    "counts = np.zeros((len(signals), len(indices)))\n",
    "for i, signal in tqdm.notebook.tqdm(enumerate(signals), total=len(signals)):\n",
    "    idx, cts = np.unique(signals[i], return_counts=True)\n",
    "    counts[i][idx] = cts\n",
    "probs = counts/signals.shape[1]\n",
    "assert np.allclose(probs.sum(axis=-1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63952a24",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# plot mean bootstrapped PMF against true PMF\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(indices, ps)\n",
    "plt.bar(indices, probs.mean(axis=0), yerr=probs.std(axis=0), width=0.5)\n",
    "plt.legend([\"Generating PMF\", \"Mean Boot PMF\"])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c10d10",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a53ea6",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# generates PMFs from blocked bootstrap of data signal\n",
    "def blocked_bootstrap_pmfs(\n",
    "    data, # this is the data signal (1-D array)\n",
    "    n_blocks_per_signal, # this determines block size (choose carefully)\n",
    "    n_blocks = 1000, # should be a multiple of n_blocks_per_signal\n",
    "    n_resamples = 9999, # number of resampled signals to construct\n",
    "):\n",
    "    # assess input signal values are valid indices\n",
    "    indices, truth = np.unique(data, return_counts=True)\n",
    "    assert np.all(indices == np.arange(len(indices))), \"The input values must be indices of the states they represent!\"\n",
    "\n",
    "    # generate blocks from signal\n",
    "    block_len = len(data)//n_blocks_per_signal\n",
    "    padded_samples = np.concatenate([\n",
    "        data, # support wrap around by adding an extra block length:\n",
    "        data[:block_len+(-len(data)%block_len)]\n",
    "    ])\n",
    "    blocks = np.concatenate([\n",
    "        np.split(padded_samples[:-block_len], (len(padded_samples)-1)//block_len)[:-1]\n",
    "        for i in range(n_blocks//n_blocks_per_signal) for j in rng.integers(block_len, size=1)\n",
    "    ])\n",
    "\n",
    "    # generate resampled signals\n",
    "    signals = np.concatenate(\n",
    "        blocks[\n",
    "            rng.integers(len(blocks), size=n_blocks_per_signal*n_resamples)\n",
    "        ].reshape(n_blocks_per_signal, n_resamples, -1) # indexing blocks is the slow step of this cell\n",
    "    , axis=1)\n",
    "\n",
    "    # compile counts for each signal\n",
    "    counts = np.zeros((len(signals), len(indices)))\n",
    "    for i, signal in tqdm.notebook.tqdm(enumerate(signals), total=len(signals)):\n",
    "        idx, cts = np.unique(signals[i], return_counts=True)\n",
    "        counts[i][idx] = cts\n",
    "\n",
    "    # return discrete distributions for each resample\n",
    "    probs = counts/signals.shape[1]\n",
    "    assert np.allclose(probs.sum(axis=-1), 1)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1898d62",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# plot computed stds against analytic ones\n",
    "n_samples, ps = 10000, simplex(1, 20)[0]\n",
    "probs = blocked_bootstrap_pmfs(rng.choice(np.arange(len(ps)), size=n_samples, p=ps), 100)\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(*[(0, probs.std(axis=0).max())]*2, c='k', ls='--', lw=1)\n",
    "plt.scatter(probs.std(axis=0), dG_err(ps, n_samples)[0])\n",
    "plt.xlabel(\"blocked bootstrap\")\n",
    "plt.ylabel(\"analytic\")\n",
    "plt.tight_layout()\n",
    "print(\"NOTE: The error between bootstrapped and analytic is most strongly a function of block size!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcef6c2",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
